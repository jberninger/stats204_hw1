---
title: "Stats 204 | Data Analysis"
author: "Jordan Berninger"
date: "10/3/2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(MASS)
library(tidyr)
```

## Homework 1

Chapter 1: 2, 3, 6, 7, 8, 10, 11, 12, 14

Chapter 2: 3, 4, 5, 6, 7, 8, 12, 13

Problem 1.2:

Use the $curve$ function to display the graph of the $\chi^2(1)$ density. The chi-square density function is $dchisq$.

```{r}
curve( dchisq(x, df=1), col='green', ylab = "Probability Density")
title("Chi-Squared Distribution (df = 1)")
```


Problem 1.3:

Use the curve function to display the graph of the gamma density with shape parameter 1 and rate parameter 1. Then use the curve function with add=TRUE to display the graphs of the gamma density with shape parameter k and rate 1, 2, 3, all in the same graphics window. The gamma density function is dgamma. Consult the help file ?dgamma to see how to specify the parameters.

I am interested in both parameters, so  I will change the rate first and the shape second.

```{r}
plot(x = c(), y = c(), xlim = c(0,3), ylim = c(0,3), ylab = "Probability Density")
curve( dgamma(x, shape = 1, rate = 1), col=1, add=TRUE)
curve( dgamma(x, shape = 1, rate = 2), col=2, add=TRUE)
curve( dgamma(x, shape = 1, rate = 3), col=3, add=TRUE)
#ylabel("Probability Density", add=TRUE)
title("Gamma Distribution")
legend("topright", c("1", "2", "3"), col = 1:3, pch = 15, inset = 0.05, title = 'rate')
```

Now, for fun, I will play around with the shape parameter. Note that the x and y axes have reset in this second plot.

```{r}
plot(x = c(), y = c(), xlim = c(0,5), ylim = c(0,1), ylab = "Probability Density")
curve( dgamma(x, shape = 1, rate = 1), col=1, add=TRUE)
curve( dgamma(x, shape = 2, rate = 1), col=2, add=TRUE)
curve( dgamma(x, shape = 3, rate = 1), col=3, add=TRUE)
title("Gamma Distribution")
legend("topright", c("1", "2", "3"), col = 1:3, pch = 15, inset = 0.05, title = 'Shape')
```



Problem 1.6:

Refer to Example 1.2 where the heights of the United States Presidents are compared with their main opponent in the presidential election. Create a scatterplot of the loser’s height vs the winner’sheight using the $plot$ function. Compare the plot to the more detailed plotshown in the Wikipedia article “Heights of Presidents of the United States and presidential candidates” [54]

The plot in the Wikipedia article has more data points and it also draws some handy lines, it is interesting to see who defeated opponents that were taller than them, and what percentage of tallest candidates win.

```{r cars}
winner = c(185, 182, 182, 188, 188, 188, 185, 185, 177, 182, 182, 193, 183, 179, 179, 175)
opponent = c(175, 193, 185, 187, 188, 173, 180, 177, 183,  185, 180, 180, 182, 178, 178, 173)
df <- data.frame(winner, opponent)
ggplot(data = df, aes(x = winner, y = opponent)) + geom_point() + ggtitle("Heights (cm)")
```

Problem 1.7:

The $rpois$ function generates random observations from a Poisson distribution. In Example 1.3, we compared the deaths due to horsekicks to a Poisson distribution with mean$\lambda=0.61$, and in Example 1.4 we simulated random Poisson($\lambda=0.61$) data. Use the $rpois$ function to simulate very large $(n= 1000 , n= 10000)$ Poisson($\lambda=0.61$) random samples. Find the frequency distribution, mean and variance for the sample. Compare the theoretical Poisson density with the sample proportions (see Example 1.4).


We will do this first for $n = 1000$ and later for $n = 10000$. First we return the frequency distribution of the sample:

```{r}
y = rpois(1000, lambda=.61)
kicks = table(y)   #table of sample frequencies
kicks / 1000
```

Now, we get the mean and variance of the sample distribution and we note that both of these are close to $0.61$, the value we set for the parameter in the $rpois$ function. We also recall that the Poisson distribution has the same parameter for the mean and variance. 

```{r}
mean(y)
```

```{r}
var(y)
```

This is what we would expect to see from a true random poisson number generator.

Finally, we compare the Theoretical and Sample distributions.

```{r}

Theoretical = dpois(0:4, lambda = .61)
Sample = kicks / 1000
cbind(Theoretical, Sample)

```

These distribution are fairly similar. We see that the Theoretical disttibution gets very low and if we continued, values higher than 4 would have non-zero and decreasing probabilities.


Problem 1.8:

Refer to Example 1.3. Using the $ppois$ function, compute the cumulative distribution function (CDF) for the Poisson distribution with mean$\lambda=0.61$, for the values 0 to 4. Compare these probabilities with the empirical CDF. The empirical CDF is the cumulative sum ofthe sample proportions p, which is easily computed using the $cumsum$ function. Combine the values of 0:4, the CDF, and the empirical CDF in a matrix to display these results in a single table.

We note that the theoretical distribution in this table does not sum exactly to one, since there is a probability that a larger number of events occurs. However, the Sample distribution in this table sums exactly to 1. This means that in our sample poisson distribution we did not see any occurances of values higher than 4 - these values are very unlikely so that makes sense.

```{r}
Theoretical = ppois(c(0:4), lambda = .61)
Empirical = cumsum(kicks/sum(kicks))
cbind(Theoretical, Empirical)
```

Problem 1.10


1.10 (Euclidean norm function). Write a function norm that will compute the Euclidean norm of a numeric vector. The Euclidean norm of a vector.

```{r}
norm <- function(x){
  sqrt(apply(x^2, 1, sum))
}
norm(matrix(c(0,0,0,1), nrow = 1))
norm(matrix(c(2,5,2,4), nrow = 1))
```


Problem 1.11

Use the $curve$ function to display the graph of the function $f(x) = e-x^2/(1+x^2)$ on the interval $0 \leq x \leq 10$. Then
use the $integrate$ function to compute the value of the integral

```{r}
curve(expr = exp(-x^2)/(1+x^2), from = 0, to = 10)
title("Problem 1.11 Graph of the function")
```

```{r}
integrate(f = function(x){exp(-x^2)/(1+x^2)}, lower = 0, upper = Inf)
```


Problem 1.12

Construct a matrix with 10 rows and 2 columns, containing random standard normal data. This is a random sample of 10 observations from a standard bivariate normal distribution. Use the apply function and your norm function from Exercise 1.10 to compute the Euclidean norms for each of these 10 observations.

I built the the ability to iterate on rows into my $norm$ function so I don't have to use $apply$ here.

```{r}
x = matrix(rnorm(20), 10, 2)
norm <- function(x){
  sqrt(apply(x^2, 1, sum))
}
norm(x)
```


Problem 1.14

The following data describe the tearing factor of paper manufactured under different pressures during pressing. The data is given in Hand et al. [21, Page 4]. Four sheets of paper were selected and tested from each of the five batches manufactured

```{r}
pressure <- c(rep(35.0, 4), rep(49.5, 4), rep(70.0, 4), rep(99.0, 4), rep(140.0, 4))
tear <- c(112,119,17,113, 108,99,112,118, 120,106,102,109, 110,101,99,104, 100,102,96,101)
data.frame(pressure, tear)
```


Problem 2.3 (pdf p 75)

Display the mtcars data included with R and read the documentation using ?mtcars. Display parallel boxplots of the quantitative variables. Display a pairs plot of the quantitative variables. Does the pairs plot reveal any possible relations between the variables?

The following boxplot shows that the quantitative variables are using different scales, which it to be expected. 

```{r}
data(mtcars)
ggplot(data = gather(mtcars), aes(y = value, fill = key)) + geom_boxplot() + 
  facet_grid(~key) + 
  theme(axis.text.x = element_text(angle = 90))
```

The pairwise scatterplots indicates that several variables are correlated with $mpg$, specifically, $disp$, $hp$, and $wt$ seems to have negative correlations with $mpg$, while $drat$ and $qsec$ appear to have positive correlations with $mpg$. Furthermore, the scatterplots show that $vs$ and $am$ only have 2 values, while $gear$ has 3 values.

```{r}
pairs(mtcars)
```


It will also be nice to look at the log transformed boxplots as well, since the previous one was smushed. the log transformed box plot shows that several of the variables have similar ranges, specifically, $drat$, $gearA$ and $wt$ have very similar distributions.

```{r, warning=FALSE}
ggplot(data = gather(mtcars), aes(y = log(value), fill = key)) + geom_boxplot() + facet_grid(~key) +
  ggtitle("log(Transformed) values") + theme(axis.text.x = element_text(angle = 90))
```


Problem 2.4

Refer to Example 2.7. Create a new variable r equal to the ratio of brain size over body size. Using the full mammals data set, order the mammals data by the ratio r. Which mammals have the largest ratios of brain size to body size? Which mammals have the smallest ratios? (Hint: use head and tail on the ordered data.)

We will start by listing the mammals with the highest ratios of brain to body size using the $head()$ function on the ordered data.

```{r}

data(mammals)
mammals %>% mutate(brain_to_body = brain/body,
                   names = row.names(mammals)) %>%
  arrange(desc(brain_to_body)) %>%
  head()
```


Now, we will list the mammals with the lowest ratios of brain to body size using the $tail()$ function on the ordered data.

```{r}

mammals %>% mutate(brain_to_body = brain/body,
                   names = row.names(mammals)) %>%
  arrange(desc(brain_to_body)) %>%
  tail()
```


Problem 2.5

Refer to Exercise 2.5. Construct a scatterplot of the ratio $r = brain/body$ vs body size for the full mammals data set.

```{r}
m1 <- mammals %>% mutate(brain_to_body = brain/body,
                         names = row.names(mammals))
ggplot(data = m1, aes(x = body, y = brain_to_body)) + geom_point()
```


Problem 2.6

The LakeHuron data contains annual measurements of the level, in feet, of Lake Huron from 1875 through 1972. Display a time plot of the data. Does the average level of the lake appear to be stable or changing with respect to time? Refer to Example 2.4 for a possible method of transforming this series so that the mean is stable, and plot the resulting series. Does the transformation help to stabilize the mean?


First we show the original data, which appears to neither mean nor variance constant with respect to time. 

```{r}
data(LakeHuron)
plot.ts(LakeHuron)
lines(lowess(LakeHuron))
```

Now we show the differenced series, we appears to be more 'stable' over time, meaning its mean and variance appear to be consistent across different time windows:

```{r}
plot.ts(diff(LakeHuron))
```


Problem 2.7

Refer to Example 2.6, where we computed sample means for each row of the $randu$ data frame. Repeat the analysis, but instead of $randu$, create a matrix of random numbers using $runif$.

If this is a true random number generator, then we expect the column means to be $\frac{1}{2}$ and the variance of the columns to be $\frac{1}{2}$. We will check this by returning the column means and the diagonal values of the variance-covariance matrix.

```{r}
x <- runif(400, 0, 1)
y <- runif(400, 0, 1)
z <- runif(400, 0, 1)
mat <- cbind(x, y, z)
colMeans(mat)
```

```{r}
diag(var(mat))
```

Here, we see that the values are fairly close to the expected values. Also, if these are truly randomly generated, we would expect the vectors to have zero or low correlation. We will check the correlation matrix's off daigonal values for low values:

```{r}
cor(mat)
```

The correlation matrix shows the expected values in the off-diagonals, which is good for a random number generator. Now we will look at the 3-D plot.

```{r}
library(lattice)
cloud(z ~ x + y, data=data.frame(mat))
```

In this 3D plot, the data points appear to be randomly distributed and there are no apparent patterns in the data, which is good for a random number generator. Now we will plot the row means of our matrix, giving us 400 sample means from 3 data points each. The following histogram appears symmetric, centered on $0.5$ in an apparent bell shape. According to the Central Limit Theorem, we should expect the sample means to form a Gaussian distribution, centered on the population mean. We see that the sample distribution follows an approximately Gaussian distribution in the four following plots. These all indicate that $runif$ is a good random number generator.

```{r}
par(mfrow = c(2,2))
hist(rowMeans(mat))
plot(density(rowMeans(mat)))
truehist(rowMeans(mat))
curve(dnorm(x, 1/2, sd=sqrt(1/36)), add=TRUE)
qqnorm(rowMeans(mat))
qqline(rowMeans(mat))
```


Problem 2.8

Refer to Example 2.6 and Exercise 2.7, where we computed sample means for each row of the data frame. Repeat the analysis in Exercise 2.7, but instead of sample size 3 generate a matrix that is 400 by 10 (sample size 10). Compare the histogram for sample size 3 and sample size 10. What does the Central Limit Theorem tell us about the distribution of the mean as sample size increases?

We create a larger matrix and note that the sample means and sample variances are approximate $\frac{1}{2}$ and $\frac{1}{12}$, respectively

```{r}
mat2 <- matrix(replicate(10, runif(400, 0, 1)), ncol = 10)
colMeans(mat2)
diag(var(mat2))
```

We will not print the correlation matrix in this case since it is 10x10 and a little too big, but I did confirm that the off-diagonal values are approximately $0$. We cannot make the 3D plot in this case since their are 10 numbers in each sample. We generate analgous plots for the sample size 10 case and see that the sample means form an approximately normal distribution

```{r}
par(mfrow = c(2,2))
hist(rowMeans(mat2))
plot(density(rowMeans(mat2)))
truehist(rowMeans(mat2))
curve(dnorm(x, 1/2, sd=sqrt(1/36)), add=TRUE)
qqnorm(rowMeans(mat2))
qqline(rowMeans(mat2))
```

The Central Limit Theorem tells us that as we take an increasing number of samples from a population, the sample means will tend towards the normal distribution centered on the true population mean. We inspect the sample size 3 and sample size 10 histograms side by side.


```{r}
par(mfrow = c(1,2))
hist(rowMeans(mat), main = "Sample size 3")
hist(rowMeans(mat2), main = "Sample size 10")
```


The histograms from the plot with sample size 10 shows a more sharply tapered bell curve than the sample size 3 counterpart. The plots also indicate that the sample size 10 histogram is more firmly centered at $\frac{1}{2}$. We can also note that the range of row means is smaller for the size 10 sample. We can see in this small example that the larger the random sample, the more normal / Gaussian the sample means appear to be.

Problem 2.12

Refer to Example 2.1. Using the full mammals data set, order the data by brain size. Which mammals have the largest brain sizes? Which mammals have the smallest brain sizes?

First the largest brains (from largest to smallest):

```{r}
mammals %>% mutate(names = row.names(mammals)) %>% 
  arrange(desc(brain)) %>% 
  head()
```


Now the smallest brains (from smallest to largest):

```{r}
mammals %>% mutate(names = row.names(mammals)) %>% 
  arrange(brain) %>% 
  head()
```

Problem 2.13

Refer to the mammals data in Example 2.7. Construct a scatterplot like Figure 2.19 on the original scale (Figure 2.19 is on the log-log scale.) Label the points and distances for cat, cow, and human. In this example, which plot is easier to interpret?

I believe the plot with log transformations on both axes is superior and more informative since we have less congenstion in the bottom left of the plot. In the plot with log transformations on both axes, there is greater separation between the points and we can get more insight on how the different species cluster and the relationship between the 2 variables.

```{r}
plot(mammals$body, mammals$brain, xlab="body", ylab="brain", ylim = c(-100, 6000))
y = mammals[c("Cat", "Cow", "Human"), ]
polygon(y)
text(y, rownames(y), adj=c(0.0, 1.5))
```
